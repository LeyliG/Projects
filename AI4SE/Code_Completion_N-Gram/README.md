# Code completion with N-Gram Probabilistic Language Modeling

Author: Leyli (Aya) Garryyeva

Goal: Implement probabilistic language model N-Gram to assist with code completion for Java source code. 

## Installation

### Set up the environment 

```bash
conda create -n java_ngram python=3.11

conda activate java_ngram

pip install javalang scikit-learn

```

### Running the model

```bash
cd Code_Completion_N-Gram

python3 model.py

```

The N-Gram model learns patterns in Java code structure and syntax, allowing it to perform a code completions task. Here are its key features:

- **Tokenization**: We convert the Java source code (classes) into tokens using the _javalang_ library.
- **N-gram size**: We allow n-gram sizes from 1 to 5 for experimentation with different context lengths.
- **Conditional probability**: We use conditional frequency distributions to predict the next token based on the previous n-1 tokens.
- **Vocabulary**:  We create a vocabulary from the training data, handling out-of-vocabulary words with an <UNK> token.
- **Data splitting**: We divide the dataset into training, validation, and test sets.
- **Evaluation**: We calculate perplexity on the validation set to measure model performance.
- **Code completion**: We generate code completions based on a given context.

Note: We are applying the model only to those instances that have <100 code tokens. 

Preliminary observations: Although 5-gram has higher perplexity value the code generated by the model is the most complete compared to the other context sizes. 


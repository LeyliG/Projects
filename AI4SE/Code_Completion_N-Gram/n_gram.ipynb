{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pydriller javalang nltk pygments pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.lm import MLE, Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, everygrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pydriller import Repository\n",
    "import javalang\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from javalang.parse import parse\n",
    "from javalang.tree import MethodDeclaration\n",
    "\n",
    "from pygments.lexers.jvm import JavaLexer\n",
    "from pygments.lexers import get_lexer_by_name\n",
    "from pygments.token import Token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Java Methods from Repositories\n",
    "1. Use PyDriller to traverse commits in the master branch of each repo.\n",
    "2. Extract method names and their source code using javalang.\n",
    "3. Store the extracted data in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.github.com/dustin/java-memcached-client',\n",
       " 'https://www.github.com/davidb/scala-maven-plugin',\n",
       " 'https://www.github.com/cyberfox/jbidwatcher']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names = \"\"\"\n",
    "dustin/java-memcached-client\n",
    "davidb/scala-maven-plugin\n",
    "cyberfox/jbidwatcher\n",
    "\"\"\"\n",
    "\n",
    "repoList = [\"https://www.github.com/\" + repo for repo in repo_names.strip().split('\\n')]\n",
    "repoList[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing commit: c0772446a20d62bcaf7104527c0ea512b817a7b1\n",
      "Processing commit: afe8e17710e8dc5993aadaf6043afdb281a1cbdd\n",
      "Extracted methods from MemcachedClient.java in commit afe8e17710e8dc5993aadaf6043afdb281a1cbdd\n",
      "Extracted methods from MemcachedConnection.java in commit afe8e17710e8dc5993aadaf6043afdb281a1cbdd\n",
      "Extracted methods from ConnectionFactoryBuilderTest.java in commit c232307ad8e0c7ccc926e495dd7d5aad2d713318\n",
      "Processing commit: fd1447f38f4db8b719387366419fe476df01957d\n",
      "https://www.github.com/dustin/java-memcached-client\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA EXTRACTION -------------------------------\n",
    "def extract_methods_from_java(code):\n",
    "    \"\"\"\n",
    "    Extract methods from Java source code using javalang parser.\n",
    "\n",
    "    Args:\n",
    "        code (str): The Java source code.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing method names and their full source code.\n",
    "    \"\"\"\n",
    "    methods = []\n",
    "    try:\n",
    "        # Parse the code into an Abstract Syntax Tree (AST)\n",
    "        tree = javalang.parse.parse(code)\n",
    "        lines = code.splitlines()\n",
    "\n",
    "        # Traverse the tree to find method declarations\n",
    "        for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "            method_name = node.name\n",
    "\n",
    "            # Determine the start and end lines of the method\n",
    "            start_line = node.position.line - 1\n",
    "            end_line = None\n",
    "\n",
    "            # Use the body of the method to determine its end position\n",
    "            if node.body:\n",
    "                last_statement = node.body[-1]\n",
    "                if hasattr(last_statement, 'position') and last_statement.position:\n",
    "                    end_line = last_statement.position.line\n",
    "\n",
    "            # Extract method code\n",
    "            if end_line:\n",
    "                method_code = \"\\n\".join(lines[start_line:end_line+1])\n",
    "            else:\n",
    "                # If end_line couldn't be determined, extract up to the end of the file\n",
    "                method_code = \"\\n\".join(lines[start_line:])\n",
    "\n",
    "            methods.append((method_name, method_code))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Java code: {e}\")\n",
    "    return methods\n",
    "\n",
    "def extract_methods_to_csv_from_master(repo_path, output_csv):\n",
    "    \"\"\"\n",
    "    Extract methods from Java files in the master branch and save them in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        repo_path (str): Path to the Git repository.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Commit_Hash\", \"File_Name\", \"Method_Name\", \"Method_Code\", \"Commit_Link\"])\n",
    "\n",
    "        for commit in Repository(repo_path, only_in_branch=\"master\").traverse_commits():\n",
    "            print(f\"Processing commit: {commit.hash}\")\n",
    "\n",
    "            #We only look into the modified files. In other words, we are looking into the history of the software system by traversing each commit.\n",
    "            #Various Generative AI methods for SD have been trained on data collected in this way; for example bug fixing.\n",
    "            for modified_file in commit.modified_files:\n",
    "                if modified_file.filename.endswith(\".java\") and modified_file.source_code:\n",
    "                    methods = extract_methods_from_java(modified_file.source_code)\n",
    "\n",
    "                    for method_name, method_code in methods:\n",
    "                        commit_link = f\"{repo_path}/commit/{commit.hash}\"\n",
    "                        csv_writer.writerow([commit.hash, modified_file.filename, method_name, method_code, commit_link])\n",
    "\n",
    "                    print(f\"Extracted methods from {modified_file.filename} in commit {commit.hash}\")\n",
    "\n",
    "def extract_methods_to_csv(repo_path, output_csv):\n",
    "    \"\"\"\n",
    "    Extract methods from Java files in a repository and save them in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        repo_path (str): Path to the Git repository.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Branch_Name\", \"Commit_Hash\", \"File_Name\", \"Method_Name\", \"Method_Code\", \"Commit_Link\"])\n",
    "\n",
    "        branch_name = \"master\"\n",
    "        for commit in Repository(repo_path, only_in_branch=branch_name).traverse_commits():\n",
    "            print(f\"Processing commit: {commit.hash}\")\n",
    "\n",
    "            for modified_file in commit.modified_files:\n",
    "                if modified_file.filename.endswith(\".java\") and modified_file.source_code:\n",
    "                    methods = extract_methods_from_java(modified_file.source_code)\n",
    "\n",
    "                    for method_name, method_code in methods:\n",
    "                        commit_link = f\"{repo_path}/commit/{commit.hash}\"\n",
    "                        csv_writer.writerow([branch_name, commit.hash, modified_file.filename, method_name, method_code, commit_link])\n",
    "\n",
    "                    print(f\"Extracted methods from {modified_file.filename} in commit {commit.hash}\")\n",
    "\n",
    "\n",
    "\n",
    "for repo in repoList[0:1]:\n",
    "\n",
    "    fileNameToSave = ''.join(repo.split('github.com')[1:])\n",
    "    fileNameToSave = fileNameToSave.replace('/','_')\n",
    "\n",
    "    # Specify the path to the output CSV file\n",
    "    output_csv_file = \"extracted_methods_{}.csv\".format(fileNameToSave)\n",
    "    # Run the extraction\n",
    "    extract_methods_to_csv_from_master(repo, output_csv_file)\n",
    "\n",
    "\n",
    "    print(repo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess the Extracted Data\n",
    "1. Remove duplicates (Type-1 Clones)\n",
    "2. Filter ASCII methods (ensure only valid characters are retained)\n",
    "3. Remove outliers based on method length (5th-95th percentile filtering)\n",
    "4. Remove boilerplate methods (setters/getters)\n",
    "5. Remove comments using Pygments lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commit_Hash</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Method_Name</th>\n",
       "      <th>Method_Code</th>\n",
       "      <th>Commit_Link</th>\n",
       "      <th>Method_Java_No_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afe8e17710e8dc5993aadaf6043afdb281a1cbdd</td>\n",
       "      <td>MemcachedClient.java</td>\n",
       "      <td>storeAsync</td>\n",
       "      <td>\\tpublic void storeAsync(StoreOperation.StoreT...</td>\n",
       "      <td>https://www.github.com/dustin/java-memcached-c...</td>\n",
       "      <td>\\tpublic void storeAsync(StoreOperation.StoreT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afe8e17710e8dc5993aadaf6043afdb281a1cbdd</td>\n",
       "      <td>MemcachedClient.java</td>\n",
       "      <td>storeSync</td>\n",
       "      <td>\\tpublic String storeSync(StoreOperation.Store...</td>\n",
       "      <td>https://www.github.com/dustin/java-memcached-c...</td>\n",
       "      <td>\\tpublic String storeSync(StoreOperation.Store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afe8e17710e8dc5993aadaf6043afdb281a1cbdd</td>\n",
       "      <td>MemcachedClient.java</td>\n",
       "      <td>storeResult</td>\n",
       "      <td>\\t\\t\\t\\t\\tpublic void storeResult(String val) ...</td>\n",
       "      <td>https://www.github.com/dustin/java-memcached-c...</td>\n",
       "      <td>\\t\\t\\t\\t\\tpublic void storeResult(String val) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Commit_Hash             File_Name  \\\n",
       "0  afe8e17710e8dc5993aadaf6043afdb281a1cbdd  MemcachedClient.java   \n",
       "1  afe8e17710e8dc5993aadaf6043afdb281a1cbdd  MemcachedClient.java   \n",
       "2  afe8e17710e8dc5993aadaf6043afdb281a1cbdd  MemcachedClient.java   \n",
       "\n",
       "   Method_Name                                        Method_Code  \\\n",
       "0   storeAsync  \\tpublic void storeAsync(StoreOperation.StoreT...   \n",
       "1    storeSync  \\tpublic String storeSync(StoreOperation.Store...   \n",
       "2  storeResult  \\t\\t\\t\\t\\tpublic void storeResult(String val) ...   \n",
       "\n",
       "                                         Commit_Link  \\\n",
       "0  https://www.github.com/dustin/java-memcached-c...   \n",
       "1  https://www.github.com/dustin/java-memcached-c...   \n",
       "2  https://www.github.com/dustin/java-memcached-c...   \n",
       "\n",
       "                             Method_Java_No_Comments  \n",
       "0  \\tpublic void storeAsync(StoreOperation.StoreT...  \n",
       "1  \\tpublic String storeSync(StoreOperation.Store...  \n",
       "2  \\t\\t\\t\\t\\tpublic void storeResult(String val) ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------- DATA CLEANING -------------------------------\n",
    "### Type 1 Clones ###\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Remove duplicate methods based on method content.\n",
    "      Almost Type-1 with the exception of comments\n",
    "    \"\"\"\n",
    "    return data.drop_duplicates(subset=\"Method_Code\", keep=\"first\")\n",
    "\n",
    "def filter_ascii_methods(data):\n",
    "    \"\"\"Filter methods to include only those with ASCII characters.\"\"\"\n",
    "    data = data[data[\"Method_Code\"].apply(lambda x: all(ord(char) < 128 for char in x))]\n",
    "    return data\n",
    "\n",
    "# Three Approaches:\n",
    "# \t1.\tData Distribution-Based Filtering: We eliminate outliers by analyzing the original data distribution, as demonstrated below.\n",
    "# \t2.\tLiterature-Driven Filtering: We follow best practices outlined in research, such as removing methods exceeding 512 tokens in length.\n",
    "# \t3.\tHybrid Approach: We combine elements from both the distribution-based and literature-driven methods.\n",
    "\n",
    "def remove_outliers(data, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Remove outliers based on method length.\"\"\"\n",
    "    method_lengths = data[\"Method_Code\"].apply(len)\n",
    "    lower_bound = method_lengths.quantile(lower_percentile / 100)\n",
    "    upper_bound = method_lengths.quantile(upper_percentile / 100)\n",
    "    return data[(method_lengths >= lower_bound) & (method_lengths <= upper_bound)]\n",
    "\n",
    "\n",
    "def remove_boilerplate_methods(data):\n",
    "    \"\"\"Remove boilerplate methods like setters and getters.\"\"\"\n",
    "    boilerplate_patterns = [\n",
    "        r\"\\bset[A-Z][a-zA-Z0-9_]*\\s*{\",  # Setter methods\n",
    "        r\"\\bget[A-Z][a-zA-Z0-9_]*\\s*{\",  # Getter methods\n",
    "    ]\n",
    "    boilerplate_regex = re.compile(\"|\".join(boilerplate_patterns))\n",
    "    data = data[~data[\"Method_Code\"].apply(lambda x: bool(boilerplate_regex.search(x)))]\n",
    "    return data\n",
    "\n",
    "def remove_comments_from_dataframe(df: pd.DataFrame, method_column: str, language: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes comments from Java methods in a DataFrame and adds a new column with cleaned methods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the methods.\n",
    "        method_column (str): Column name containing the raw Java methods.\n",
    "        language (str): Programming language for the lexer (e.g., 'java').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new column 'Java Method No Comments'.\n",
    "    \"\"\"\n",
    "    # Define a function to remove comments from a single method\n",
    "    def remove_comments(code):\n",
    "        lexer = get_lexer_by_name(language)\n",
    "        tokens = lexer.get_tokens(code)\n",
    "        # Filter out comments using a lambda function\n",
    "        clean_code = ''.join(token[1] for token in tokens if not (lambda t: t[0] in Token.Comment)(token))\n",
    "\n",
    "\n",
    "        return clean_code\n",
    "\n",
    "    # Apply the function to the specified column and add a new column with the results\n",
    "    df[\"Method_Java_No_Comments\"] = df[method_column].apply(remove_comments)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_methods(data):\n",
    "    data = remove_duplicates(data)\n",
    "    data = filter_ascii_methods(data)\n",
    "    data = remove_outliers(data)\n",
    "    data = remove_boilerplate_methods(data)\n",
    "    data = remove_comments_from_dataframe(data, \"Method_Code\", \"java\")\n",
    "    return data\n",
    "\n",
    "# Load the extracted methods from the CSV file\n",
    "data = pd.read_csv(\"extracted_methods__dustin_java-memcached-client.csv\")\n",
    "# Clean the data\n",
    "data = clean_methods(data)\n",
    "# Display the cleaned data\n",
    "data.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train n-gram Models for Code Completion\n",
    "1. Tokenize the Java method source code using Pygments.\n",
    "2. Build n-gram language models for n=3 and n=5 using NLTK or KenLM.\n",
    "3. Train on extracted and cleaned methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- N-GRAM MODELING -------------------------------\n",
    "def tokenize_java_code(code):\n",
    "    \"\"\"Tokenize Java code using Pygments lexer.\"\"\"\n",
    "    lexer = JavaLexer()\n",
    "    tokens = [t[1] for t in lexer.get_tokens(code) if t[0] not in Token.Comment]  # Exclude comments\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def train_ngram_model(methods, n, smoothing=\"laplace\"):\n",
    "    tokenized_methods = [tokenize_java_code(method) for method in methods]\n",
    "    train_data, vocab = padded_everygram_pipeline(n, tokenized_methods)\n",
    "    \n",
    "    model = Laplace(n) if smoothing == \"laplace\" else MLE(n)\n",
    "    model.fit(train_data, vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate and Select the Best Model\n",
    "1. Test both models on 100 Java methods.\n",
    "2. Measure perplexity (lower is better) or accuracy (prediction correctness).\n",
    "3. Select the best-performing model for final use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, test_methods, n):\n",
    "    test_tokenized = [tokenize_java_code(method) for method in test_methods]\n",
    "    test_ngrams = [list(everygrams(tokens, max_len=n)) for tokens in test_tokenized]\n",
    "    valid_ngrams = [ngrams for ngrams in test_ngrams if len(ngrams) > 0]\n",
    "\n",
    "    if not valid_ngrams:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    perplexities = [model.perplexity(ngrams) for ngrams in valid_ngrams]\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3 Perplexity: 43.40415284876779\n",
      "n=5 Perplexity: 60.023154987730514\n",
      "\n",
      "Best model: n=3 with perplexity=43.40415284876779\n",
      "\n",
      "==== CODE COMPLETION EXAMPLE ====\n",
      "🎯 Ground Truth:\n",
      "   public V remove(Object key) {\n",
      "    V rv = null;\n",
      "    try {\n",
      "      rv = get(key);\n",
      "      client.delete(getKey((String) key));\n",
      "    } catch (ClassCastException e) {\n",
      "      // Not a string key. Ignore.\n",
      "    } ...\n",
      "🤖 Generated Code:\n",
      "    public   void   testTapBucketDoesNotExist ( ) ; \n",
      " \n",
      "    boolean   waitForQueues ( long   def ) ; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- EXECUTION -------------------------------\n",
    "output_csv = \"methods.csv\"\n",
    "methods = data[\"Method_Code\"].tolist()\n",
    "\n",
    "# Step 2: Train N-Gram Models & Evaluate Perplexity\n",
    "n_values = [3, 5]\n",
    "best_n = None\n",
    "best_perplexity = float(\"inf\")\n",
    "perplexities = {}\n",
    "\n",
    "for n in n_values:\n",
    "    model = train_ngram_model(methods, n)\n",
    "    perplexity = calculate_perplexity(model, methods[:100], n)  # Using 100 Java methods for testing\n",
    "    perplexities[n] = perplexity\n",
    "\n",
    "    print(f\"n={n} Perplexity: {perplexity}\")\n",
    "    if perplexity < best_perplexity:\n",
    "        best_n = n\n",
    "        best_perplexity = perplexity\n",
    "\n",
    "print(f\"\\nBest model: n={best_n} with perplexity={best_perplexity}\")\n",
    "\n",
    "# Step 3: Generate Code vs. Ground Truth Example\n",
    "def generate_code(model, start_tokens, length=20):\n",
    "    \"\"\"Generates code based on the trained N-Gram model.\"\"\"\n",
    "    generated_tokens = start_tokens[:]\n",
    "    for _ in range(length):\n",
    "        next_word = model.generate(text_seed=generated_tokens[-2:])  # Use last two words as seed\n",
    "        if next_word is None:\n",
    "            break\n",
    "        generated_tokens.append(next_word)\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "if best_n:\n",
    "    best_model = train_ngram_model(methods, best_n)\n",
    "    example_method = random.choice(methods)\n",
    "    start_tokens = tokenize_java_code(example_method)[:2]  # Take first two words as seed\n",
    "    generated_code = generate_code(best_model, start_tokens, length=20)\n",
    "\n",
    "    print(\"\\n==== CODE COMPLETION EXAMPLE ====\")\n",
    "    print(\"🎯 Ground Truth:\\n\", example_method[:200], \"...\")\n",
    "    print(\"🤖 Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "java_ngram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

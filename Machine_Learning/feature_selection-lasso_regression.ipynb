{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learn to automate feature selection with lasso regression**\n",
    "\n",
    "Estimated time needed: **30** minutes\n",
    "\n",
    "This project is based on the <a href=\"https://developer.ibm.com/tutorials/awb-lasso-regression-automatic-feature-selection/\" target=\"_blank\" rel=\"noopener noreferrer\">IBM developer tutorial</a> by Eda Kavlakoglu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li><a href=\"#What is Lasso Regression?\">What is lasso regression?</a></li>\n",
    "    <li><a href=\"#Setup\">Setup</a></li>\n",
    "    <li><a href=\"\">Steps</a></li>\n",
    "        <ul>\n",
    "            <li><a href=\"\">Step 1. Import libraries and load the data set</a></li>\n",
    "            <li><a href=\"\">Step 2. Explore the data set</a></li>\n",
    "            <li><a href=\"\">Step 3. Split the data set</a></li>\n",
    "            <li><a href=\"\">Step 4. Standardize data points through feature scaling</a></li>\n",
    "            <li><a href=\"\">Step 5. Implement and evaluate the model</a></li>\n",
    "            <li><a href=\"\">Step 6. Optimize model with hyperparameter tuning</a></li>\n",
    "        </ul>\n",
    "    <li><a href=\"\">Summary and next steps</a></li>\n",
    "    <li><a href=\"\">Exercises</a></li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab you are able to:\n",
    " - Gain a solid understanding of regularization concepts in the context of linear regression models\n",
    " - Implement lasso regression for linear models by using Sklearn, and use grid search for hyperparameter tuning\n",
    " - Regularize linear regression models by applying lasso regression in Python to have the most predictive value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is lasso regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression, also known as L1 regularization, is a form of regularization for linear regression models. Regularization is a statistical method to reduce errors caused by overfitting on training data.\n",
    "\n",
    "Lasso stands for Least Absolute Shrinkage and Selection Operator. It's frequently used in machine learning to handle high dimensional data as it facilitates automatic feature selection. It does this by adding a penalty term to the residual sum of squares (RSS), which is then multiplied by the regularization parameter (lambda or Î»). This regularization parameter controls the amount of regularization applied. Larger values of lambda increase the penalty, shrinking more of the coefficients towards zero, which subsequently reduces the importance of (or altogether eliminates) some of the features from the model, which results in automatic feature selection. Conversely, smaller values of lambda reduce the effect of the penalty, retaining more features within the model.\n",
    "\n",
    "This penalty promotes sparsity within the model, which can help avoid issues of multicollinearity and overfitting issues within data sets. Multicollinearity occurs when two or more independent variables are highly correlated with one another, which can be problematic for causal modeling. Overfit models will generalize poorly to new data, diminishing their value altogether. By reducing regression coefficients to zero, lasso regression can effectively eliminate independent variables from the model, sidestepping these potential issues within the modeling process. Model sparsity can also improve the interpretability of the model compared to other regularization techniques such as ridge regression (also known as L2 regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you use the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data\n",
    "*   [`NumPy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine learning pipeline-related functions\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data\n",
    "*   [`Matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools\n",
    "*   [`statsmodels`](https://www.statsmodels.org/stable/index.html) for statistical data exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook's commands in a different Jupyter environment (for example, Watson Studio or Ananconda), you must install these libraries by removing the `#` sign before `!pip` in the following code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# - Update a specific package\n",
    "# !pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "# !pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm seaborn skillsnetwork scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import libraries and load the data set\n",
    "\n",
    "In this step, you import the necessary Python libraries for implementing lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import Lasso \n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "warn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "mtcars = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache=True).data\n",
    "df = pd.DataFrame(mtcars)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is originally from the 1974 Motor Trend US magazine, highlighting different attributes of car models from 1973-1974. To explore the different definitions in this data set, please check out <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\">this link</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Explore the data set\n",
    "\n",
    "Before initiating data preprocessing, you should conduct an <a href=\"https://www.ibm.com/topics/exploratory-data-analysis?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-593&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom\">exploratory data analysis</a>\n",
    " to understand the data's structure and format, including the types of variables, their distributions, and the overall organization of information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in each column of the dataset\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in any of the columns of this 11-dimensional data set, which means that you wonât have to drop any rows from the data set or impute any values.\n",
    "\n",
    "To understand whether any of the features are correlated to one another, different data visualizations, such as pair plots and correlation heatmaps, can show potential signs of multicollinearity. This can subsequently indicate the need for a dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a heatmap\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            cmap=\"BuPu\",\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            annot=True)\n",
    "plt.title(\"Correlation Heatmap of mtcars dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap computes and displays the correlation between two variables. The values range from -1 to 1, where -1 indicates perfect negative correlation, 0 indicates no correlation, and 1 denotes perfect positive correlation. In this data set, there is a strong positive correlation of 0.9 between cylinders (cyl) and displacement (disp), meaning that as one increases, the other also increases. There is also a strong negative correlation between mpg and cyl (-.85), mpg and disp (-.85), and mpg and wt (-.87). This means that as displacement, weight, and the number of cylinders increase, the fuel efficiency (mpg) tends to decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Split the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the exploratory analysis, you can conclude that there are some correlated features within the data set, and as a result, you can expect that the lasso regression model will automatically drop features to reduce the redundancy within the data set. That said, it is important to keep in mind that lasso regression has its limitations, and it will arbitrarily drop one of the correlated features from the model.\n",
    "\n",
    "From here, you split the data set into two sets, a training set and the test set. Setting the random state ensures that the splits you generate are reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[1:]\n",
    "target = df.columns[0]\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Standardize data points through feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you normalize the data to ensure that the scale of your predictors does not negatively impact variable selection as the scale of your variable does affect the size of your coefficients. Scaling your variables with a mean of zero and a standard deviation of one is a common feature scaling technique. This allows the lasso model to select the most important features more accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling and centering the data\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement and evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you apply this algorithm to your data set and return the predicted values. Itâs worth noting that youâre using `GridSearchCV` with the lasso model for this project, but you can also use `LassoCV`, which automatically incorporates cross-validation into the model.\n",
    "\n",
    "Important: While you can use `LassoCV` or `GridSearchCV` with the lasso model in scikit-learn, they wonât necessarily yield the same results. According to the documentation, `LassoCV` is warm started using the coefficients of a previous iteration of the model on the regularization path. This tends to speed up the hyperparameter search for alpha value. This is further explained in a <a href=\"https://github.com/scikit-learn/scikit-learn/issues/24877\">scikit-learn bug ticket</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize lasso regression model\n",
    "model = Lasso(max_iter=10000) #default alpha is 1\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "#Calculate R-squared\n",
    "rsquared = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {rsquared}\")\n",
    "\n",
    "#Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared and mean squared error (MSE) for this model is 0.77 and 9.26, respectively. To see which coefficients shrunk down to zero, you will plot your coefficients in both a DataFrame and a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = pd.Series(model.coef_, index=features)\n",
    "coeff\n",
    "\n",
    "alphas = np.linspace(0.01, 1000, 100)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coefs.append(model.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.legend(features)\n",
    "ax.grid(False)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title(\"Coefficient values with increasing values of alpha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your DataFrame, you see that 7 of the 10 variables have shrunk down to zero. According to this model, there are three features that are key predictors of mpg, which are the number of cylinders, horsepower, and the weight of the vehicle. However, when you observe the line graph, it looks like the model might have dropped some variables, like disp, due to high collinearity, which might have also had good predictive power. As you can see, lasso regression is helpful in reducing the number of features in a model to some of the most important ones, but it's not without its limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optimize model with hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want the lowest possible value of MSE for the optimal lasso model, and you find this by trying different values of alpha through grid search. `GridSearchCV` helps you to conduct this optimization through cross-validation, allowing you to find or confirm the best value for the alpha hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {\"alpha\": 10.0 ** np.arange(-5, 6)}\n",
    "grid_search = GridSearchCV(model, alphas, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(f\"Best value for lambda : \", grid_search.best_params_)\n",
    "print(\"Best score for cost function: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid search confirms that the default alpha value of 1 is, in fact, the optimal value for this hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you learned how to apply lasso regression to conduct automatic feature selection, which identified the subset of features in the data set that have the most predictive value to your target variable.\n",
    "\n",
    "To learn more about other supervised machine learning models that you can apply to classification and regression problems, see these tutorials in the <a href=\"https://developer.ibm.com/learningpaths/learning-path-machine-learning-for-developers\">Getting started</a> with machine learning learning path: \n",
    "- <a href=\"https://developer.ibm.com/learningpaths/learning-path-machine-learning-for-developers/learn-classification-algorithms\">Tutorial: Learn classification algorithms using Python and scikit-learn</a>\n",
    "- <a href=\"https://developer.ibm.com/learningpaths/learning-path-machine-learning-for-developers/learn-regression-algorithms\">Tutorial: Learn regression algorithms using Python and scikit-learn</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Initialize lasso regression model to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "#Initialize lasso regression model\n",
    "model = Lasso(max_iter=10000) #default alpha is 1\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - From exercise 1, get the R-squared and the MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R-squared\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "#Calculate the mean squared error\n",
    "mse_lasso = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse_lasso}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Again using `LassoCV`, get the optimal alpha and best cost function value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "    \n",
    "```python\n",
    "lasso_cv = LassoCV(cv=5)\n",
    "lasso_cv.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Print the optimal alpha parameter\n",
    "optimal_alpha = lasso_cv.alpha_\n",
    "print(\"Optimal alpha:\", optimal_alpha)\n",
    "\n",
    "# Print the corresponding cost (objective) function value\n",
    "best_cost_function_value = lasso_cv.score(X_train_scaled, y_train)\n",
    "print(\"Best cost function value:\", best_cost_function_value)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead Instructor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Eda Kavlakoglu](https://author.skills.network/instructors/eda_kavlakoglu)\n",
    "\n",
    "Eda Kavlakoglu is a Marketing leader with a technical background in data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lucy Xu](https://author.skills.network/instructors/lucy_xu)\n",
    "\n",
    "Lucy Xu is a data scientist at the Ecosystems Skills Network at IBM and a fourth year student in Statistics at the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wojciech Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk)\n",
    "\n",
    "Wojciech Fulmyk is a data scientist at the Ecosystems Skills Network at IBM and a Ph.D. candidate in Economics at the University of Calgary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
